# -*- coding: utf-8 -*-
"""fakereviewproject.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1kmoE-1R2zOyUESLyc6SQPPoJoKWWJ17E
"""

# Commented out IPython magic to ensure Python compatibility.
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
color = sns.color_palette()
# %matplotlib inline

data = pd.read_csv("/content/amazon_reviews.csv")

data.head()

data.loc[data["LABEL"] == "__label1__", "LABEL"] = '1'
data.loc[data["LABEL"] == "__label2__", "LABEL"] = '0'

len(data)

cnt_srs=data.groupby(data["LABEL"]).PRODUCT_CATEGORY.value_counts()

cnt_srs = data.groupby(data["LABEL"]).RATING.value_counts()
cnt_srs

cnt_srs = data.groupby(data["RATING"]).PRODUCT_CATEGORY.value_counts()
cnt_srs

df1 = data.groupby("LABEL").REVIEW_TEXT

data['TEXT_LENGTH'] = data['REVIEW_TEXT'].apply(len)

cnt_srs = data.groupby(["LABEL"]).TEXT_LENGTH.agg(lambda x: sum(x)/len(x))
cnt_srs

plt.figure(figsize=(16,8))
sns.barplot(cnt_srs.index, cnt_srs.values, alpha=0.8, color=color[0])
plt.ylabel('Text Length', fontsize=16)
plt.xlabel('Label', fontsize=16)
plt.title('Text length Vs Label', fontsize=18)
plt.xticks(rotation='horizontal')
plt.show()

data['num_sentences'] = data['REVIEW_TEXT'].apply(lambda x: len(str(x).split('.')))
#data['num_syllable'] = data['REVIEW_TEXT'].apply(lambda x: len(str(x).split('')))

!pip install textstat

import textstat
#from textstat import flesch_kincaid_grade

from textstat.textstat import textstat
data["FK_Score"] = data["REVIEW_TEXT"].apply(textstat.flesch_kincaid_grade)

cnt_srs = data.groupby(["LABEL"]).FK_Score.agg(lambda x: sum(x)/len(x))
cnt_srs

plt.figure(figsize=(16,8))
sns.barplot(cnt_srs.index, cnt_srs.values, alpha=0.8, color=color[0])
plt.ylabel('FKscore', fontsize=16)
plt.xlabel('Label', fontsize=16)
plt.title('FKscore Vs Label', fontsize=18)
plt.xticks(rotation='horizontal')
plt.show()

import nltk
wpt = nltk.WordPunctTokenizer()
stop_words = nltk.corpus.stopwords.words('english')

def stopCount(x):
    sum =0
    for char in x.split():
        sum+= char in stop_words
    return sum
data['stop_count'] = data['REVIEW_TEXT'].apply(stopCount)

cnt_srs = data.groupby(["LABEL"]).stop_count.agg(lambda x: sum(x)/len(x))
cnt_srs

plt.figure(figsize=(16,8))
sns.barplot(cnt_srs.index, cnt_srs.values, alpha=0.8, color=color[0])
plt.ylabel('Stopword counts', fontsize=16)
plt.xlabel('Label', fontsize=16)
plt.title('Stopwords Counts Vs Label', fontsize=18)
plt.xticks(rotation='horizontal')
plt.show()

def capsCount(x):
    sum =0
    for char in x:
        sum+= char in "QWERTYUIOPASDFGHJKLZXCVBNM"
    return sum
data['caps_count'] = data['REVIEW_TEXT'].apply(capsCount)

cnt_srs = data.groupby(["LABEL"]).caps_count.agg(lambda x: sum(x)/len(x))
cnt_srs

plt.figure(figsize=(16,8))
sns.barplot(cnt_srs.index, cnt_srs.values, alpha=0.8, color=color[0])
plt.ylabel('Caps_count', fontsize=16)
plt.xlabel('Label', fontsize=16)
plt.title('Caps_count Vs Label', fontsize=18)
plt.xticks(rotation='horizontal')
plt.show()

import string
count = lambda l1,l2: sum([1 for x in l1 if x in l2])
def punctCount(x):
    return count(x, set(string.punctuation))
data['punct_count'] = data['REVIEW_TEXT'].apply(punctCount)

cnt_srs = data.groupby(["LABEL"]).punct_count.agg(lambda x: sum(x)/len(x))
cnt_srs

plt.figure(figsize=(16,8))
sns.barplot(cnt_srs.index, cnt_srs.values, alpha=0.8, color=color[0])
plt.ylabel('Punctuation_count', fontsize=16)
plt.xlabel('Label', fontsize=16)
plt.title('Punctuation_count Vs Label', fontsize=18)
plt.xticks(rotation='horizontal')
plt.show()

import re
import string
match_list = []

def checkName(title,text):
    matches = []
    for word in title.split():
        #removing punctuation
        word = "".join((char for char in word if char not in string.punctuation))
        #print(word)
        myreg = r'\b'+word+r'\b'
        r = re.compile(myreg, flags=re.I | re.X)
        matches.append(r.findall(text))
    return len(matches)


for a,b in zip(data.PRODUCT_TITLE, data.REVIEW_TEXT):
    number_of_matches = checkName(a,b)
    match_list.append(number_of_matches)

data["matchesDf"] = match_list

cnt_srs = data.groupby(["LABEL"]).matchesDf.agg(lambda x: sum(x)/len(x))
cnt_srs

plt.figure(figsize=(16,8))
sns.barplot(cnt_srs.index, cnt_srs.values, alpha=0.8, color=color[3])
plt.ylabel('ProductName_Matches_count', fontsize=16)
plt.xlabel('Label', fontsize=16)
plt.title('ProductName_Matches_count Vs Label', fontsize=18)
plt.xticks(rotation='horizontal')
plt.show()

data["emojis"] = data["REVIEW_TEXT"].apply(lambda x: 1 if ";)" in x.split() or ":)" in x.split() or ":-)" in x.split() else 0)

cnt_srs = data.groupby(["LABEL"]).emojis.agg(lambda x: sum(x))
cnt_srs

plt.figure(figsize=(16,8))
sns.barplot(cnt_srs.index, cnt_srs.values, alpha=0.8, color=color[3])
plt.ylabel('Emojis_count', fontsize=16)
plt.xlabel('Label', fontsize=16)
plt.title('Emojis_count Vs Label', fontsize=18)
plt.xticks(rotation='horizontal')
plt.show()

"""====== SENTIMENT CLASSIFIER ======"""

#SENTIMENT CLASSIFIER
data.loc[data["RATING"] < 3, "RATING"] = 0
data.loc[data["RATING"] > 3, "RATING"] = 1

data.RATING.value_counts()

data1 = data.loc[data['RATING'] == 1]
print(len(data1))
data2 = data1.sample(frac=0.2, replace=True)
print(len(data2))
data3 = data1 = data.loc[data['RATING'] == 0]

data4 = pd.concat([data2, data3], ignore_index=True)

data4.head()

import csv                               # csv reader
from sklearn.svm import LinearSVC
from nltk.classify import SklearnClassifier
from random import shuffle
from sklearn.pipeline import Pipeline
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.metrics import precision_recall_fscore_support
from sklearn.metrics import accuracy_score,classification_report
import numpy as np
import nltk
import string
from nltk.corpus import stopwords
from nltk.tokenize import RegexpTokenizer
from nltk.stem import WordNetLemmatizer
from sklearn.feature_extraction.text import TfidfVectorizer
from nltk.util import ngrams
from nltk.tokenize import word_tokenize
nltk.download('punkt')

data = data4[["REVIEW_TEXT","LABEL","VERIFIED_PURCHASE", "PRODUCT_CATEGORY", "RATING"]]

data.head(2)

from sklearn.preprocessing import LabelEncoder

enc = LabelEncoder()

for col in ["VERIFIED_PURCHASE","PRODUCT_CATEGORY"]:
    data[col] = enc.fit_transform(data[col])

df = data.copy()

import os



print("Text Cleaning Begins.....")


# def replace_contractions(x):
#     for word in x.split():
#         if word in contractions:
#             return x.replace(word, contractions[word.lower()])
#     else:
#         return x

# data["Text"] = data["Text"].apply(lambda x: replace_contractions(x))

import string

PUNCT_TO_REMOVE = string.punctuation
def remove_punctuation(text):
    """custom function to remove the punctuation"""
    return text.translate(str.maketrans('', '', PUNCT_TO_REMOVE))

data["REVIEW_TEXT"] = data["REVIEW_TEXT"].apply(lambda text: remove_punctuation(text))



import nltk
nltk.download('stopwords')

from nltk.corpus import stopwords
", ".join(stopwords.words('english'))

STOPWORDS = set(stopwords.words('english'))
def remove_stopwords(text):
    """custom function to remove the stopwords"""
    return " ".join([word for word in str(text).split() if word not in STOPWORDS])

data["REVIEW_TEXT"] = data["REVIEW_TEXT"].apply(lambda text: remove_stopwords(text))



def clean_text(text):
    '''Make text lowercase, remove text in square brackets,remove links,remove punctuation
    and remove words containing numbers.'''
    text = str(text).lower()
    text = re.sub('\[.*?\]', '', text)
    text = re.sub('https?://\S+|www\.\S+', '', text)
    text = re.sub('<.*?>+', '', text)
    text = re.sub('[%s]' % re.escape(string.punctuation), '', text)
    text = re.sub('\n', '', text)
    text = re.sub('\w*\d\w*', '', text)
    return text

data["REVIEW_TEXT"] = data["REVIEW_TEXT"].apply(lambda text: clean_text(text))


from collections import Counter
cnt = Counter()
for text in data["REVIEW_TEXT"].values:
    for word in text.split():
        cnt[word] += 1

FREQWORDS = set([w for (w, wc) in cnt.most_common(10)])
def remove_freqwords(text):
    """custom function to remove the frequent words"""
    return " ".join([word for word in str(text).split() if word not in FREQWORDS])

data["REVIEW_TEXT"] = data["REVIEW_TEXT"].apply(lambda text: remove_freqwords(text))


n_rare_words = 10
RAREWORDS = set([w for (w, wc) in cnt.most_common()[:-n_rare_words-1:-1]])
def remove_rarewords(text):
    """custom function to remove the rare words"""
    return " ".join([word for word in str(text).split() if word not in RAREWORDS])

data["REVIEW_TEXT"] = data["REVIEW_TEXT"].apply(lambda text: remove_rarewords(text))

import nltk
nltk.download('wordnet')

nltk.download('averaged_perceptron_tagger')

from nltk.corpus import wordnet
from nltk.stem import WordNetLemmatizer

lemmatizer = WordNetLemmatizer()
wordnet_map = {"N":wordnet.NOUN, "V":wordnet.VERB, "J":wordnet.ADJ, "R":wordnet.ADV}
def lemmatize_words(text):
    pos_tagged_text = nltk.pos_tag(text.split())
    return " ".join([lemmatizer.lemmatize(word, wordnet_map.get(pos[0], wordnet.NOUN)) for word, pos in pos_tagged_text])

data["REVIEW_TEXT"] = data["REVIEW_TEXT"].apply(lambda text: lemmatize_words(text))



# import wordninja

# def infer_spaces(text):
#     ninja = wordninja.split(text)
#     return " ".join([word for word in ninja])

# data["REVIEW_TEXT"] = data["REVIEW_TEXT"].apply(lambda text: infer_spaces(text))

def remove_html(text):
    html_pattern = re.compile('<.*?>')
    return html_pattern.sub(r'', text)


data["REVIEW_TEXT"] = data["REVIEW_TEXT"].apply(lambda text: remove_html(text))

def remove_urls(text):
    return re.sub(r'(?i)\b((?:https?://|www\d{0,3}[.]|[a-z0-9.\-]+[.][a-z]{2,4}/)(?:[^\s()<>]+|\(([^\s()<>]+|(\([^\s()<>]+\)))*\))+(?:\(([^\s()<>]+|(\([^\s()<>]+\)))*\)|[^\s`!()\[\]{};:\'".,<>?«»“”‘’]))', '', str(text))

data["REVIEW_TEXT"] = data["REVIEW_TEXT"].apply(lambda text: remove_urls(text))


print()
print("Successfuly Cleaned :)")

data.head(1)

tfv = TfidfVectorizer(min_df= 5,max_features=2000,
            strip_accents='unicode', analyzer='word',token_pattern=r'\w{1,}',
            ngram_range=(1, 3), use_idf=1,smooth_idf=1,sublinear_tf=1,
            stop_words = 'english',)

text = tfv.fit_transform(data["REVIEW_TEXT"])

del data["REVIEW_TEXT"]

data = pd.concat([pd.DataFrame(text.toarray()),data], axis = 1)

data.shape

from sklearn.model_selection import train_test_split
Y = data["LABEL"].copy()
del data["LABEL"]
train_x, test_x, train_y, test_y = train_test_split(data,Y, test_size = 0.3, stratify = Y )

def BaseClassifier(algo, train_x,train_y,test_x,test_y):
    """
    Function: Prints Model Performance for the Baseline Classifier

    Parameters:
    -----------
    1. algo: ML or predefined algorithms
    2. train_x: Independent features to train
    3. train_y: Target variable to learn
    4. testx, test,y : Test data to get validation scores
    """
    model = algo.fit(train_x, train_y)

    print("Trained Sucessfully...")

    pred_train = model.predict(train_x)
    pred_test = model.predict(test_x)

    print()
    print ("Printing Scores:")
    print("------------------------------")

    try:
        print(f'Trained Score :\n {(accuracy_score(train_y,pred_train))}')
        print(f'Testing Score :\n {(accuracy_score(test_y,pred_test))}')
        print()
        print("Testing Classification Report:")
        print("-------------------------------")
        print(classification_report(test_y,pred_test))

    except ValueError:
        pass

    return model, pred_test

from sklearn.linear_model import LogisticRegression
logreg,predictions = BaseClassifier(LogisticRegression(),
                                    train_x,train_y,test_x,test_y)

from sklearn.svm import LinearSVC
svm,predictions = BaseClassifier(LinearSVC(C=0.01),
                                    train_x,train_y,test_x,test_y)

"""### Deep learning"""

from keras.models import Model
from keras.layers import Input, Dense, Embedding, SpatialDropout1D, add, concatenate
from keras.layers import LSTM, Bidirectional, GlobalMaxPooling1D, GlobalAveragePooling1D
from keras.preprocessing import text, sequence
from gensim.models import KeyedVectors

NUM_MODELS = 2
BATCH_SIZE = 512
LSTM_UNITS = 128
DENSE_HIDDEN_UNITS = 4 * LSTM_UNITS
EPOCHS = 4
MAX_LEN = 220

from sklearn.feature_extraction.text import CountVectorizer
from keras.preprocessing.text import Tokenizer
from keras.preprocessing.sequence import pad_sequences
from keras.models import Sequential
from keras.layers import Dense, Embedding, LSTM, SpatialDropout1D
from sklearn.model_selection import train_test_split
from keras.utils.np_utils import to_categorical
# from keras.legacy import interfaces
from keras import backend as K


from tensorflow.python.framework import ops
from tensorflow.python.ops import state_ops
from tensorflow.python.ops import control_flow_ops
from tensorflow.python.framework import constant_op
from tensorflow.python.training.optimizer import Optimizer
import tensorflow as tf
from keras.layers import Dropout

def build_matrix(word_index, path):
    embedding_index = KeyedVectors.load(path, mmap='r')
    embedding_matrix = np.zeros((len(word_index) + 1, 300))
    for word, i in word_index.items():
        for candidate in [word, word.lower()]:
            if candidate in embedding_index:
                embedding_matrix[i] = embedding_index[candidate]
                break
    return embedding_matrix

train_x.shape[1]

"""### Basic LSTM"""

def LSTM_MODEL():
    print("LSTM Network")

classifier = LSTM_MODEL()
classifier = Sequential()

# Adding the input layer and the first hidden layer
classifier.add(Dense(128, kernel_initializer = 'he_uniform',activation='linear',input_dim = 2003))
classifier.add(Dropout(0.5))

# Adding the second hidden layer
classifier.add(Dense(64, kernel_initializer = 'he_uniform',activation='tanh', ))
classifier.add(Dropout(0.5))

# Adding the second hidden layer
classifier.add(Dense(32, kernel_initializer = 'he_uniform',activation='tanh'))

# Adding the second hidden layer
classifier.add(Dense(16, kernel_initializer = 'he_uniform',activation='tanh'))
classifier.add(Dropout(0.5))

# Adding the second hidden layer
classifier.add(Dense(8, kernel_initializer = 'he_uniform',activation='tanh'))
classifier.add(Dropout(0.5))
# Adding the second hidden layer
classifier.add(Dense(4, kernel_initializer = 'he_uniform',activation='tanh'))
classifier.add(Dropout(0.5))

# Adding the output layer
classifier.add(Dense(1, kernel_initializer = 'glorot_uniform', activation = 'sigmoid'))

# Compiling the ANN
classifier.compile(optimizer = "Adam", loss = 'binary_crossentropy', metrics = ['accuracy'])
print(classifier.summary())

# Fitting the ANN to the Training set
model_history=classifier.fit(train_x,train_y.astype(int), batch_size = 8, validation_split = 0.3,epochs=30)

print(model_history.history.keys())
# summarize history for accuracy
plt.plot(model_history.history['accuracy'])
plt.plot(model_history.history['val_accuracy'])
plt.title('model accuracy')
plt.ylabel('accuracy')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='upper left')
plt.show()

# summarize history for loss
plt.plot(model_history.history['loss'])
plt.plot(model_history.history['val_loss'])
plt.title('model loss')
plt.ylabel('loss')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='upper left')
plt.show()

# Part 3 - Making the predictions and evaluating the model

# Predicting the Test set results
y_pred = classifier.predict(test_x)
y_pred = (y_pred > 0.5)

# Making the Confusion Matrix
from sklearn.metrics import confusion_matrix
cm = confusion_matrix(test_y.astype(int), y_pred)

# Calculate the Accuracy
from sklearn.metrics import accuracy_score
score=accuracy_score(y_pred,test_y.astype(int))

print("Test_Report:\n",classification_report(test_y.astype(int),y_pred))
cm

"""### ENSEMBLE LSTM"""

test_preds = np.zeros(len(test_x)).reshape(1,-1).T
max_iter = 5
for i in range(max_iter):
    data_cv = data.copy()
    data_cv["Target"] = Y.copy()

    data_cv = data_cv.sample(data_cv.shape[0])
    Y_cv = data_cv["Target"]

    del data_cv["Target"]
    classifier = LSTM_MODEL()
    classifier = Sequential()

    # Adding the input layer and the first hidden layer
    classifier.add(Dense(128, kernel_initializer = 'he_uniform',activation='linear',input_dim = 2003))

    # Adding the second hidden layer
    classifier.add(Dense(64, kernel_initializer = 'he_uniform',activation='tanh', ))

    # Adding the second hidden layer
    classifier.add(Dense(32, kernel_initializer = 'he_uniform',activation='tanh'))

    # Adding the second hidden layer
    classifier.add(Dense(16, kernel_initializer = 'he_uniform',activation='tanh'))

    # Adding the second hidden layer
    classifier.add(Dense(8, kernel_initializer = 'he_uniform',activation='tanh'))
    # Adding the second hidden layer
    classifier.add(Dense(4, kernel_initializer = 'he_uniform',activation='tanh'))
#     classifier.add(Dropout(0.5))

    # Adding the output layer
    classifier.add(Dense(1, kernel_initializer = 'glorot_uniform', activation = 'sigmoid'))

    # Compiling the ANN
    classifier.compile(optimizer = "Adam", loss = "binary_crossentropy", metrics=['accuracy'])

    # Fitting the ANN to the Training set
    model_history=classifier.fit(data_cv, Y_cv.astype(int),validation_split=0.30, batch_size = 8, epochs=5, verbose = 2)

    test_preds += classifier.predict(test_x)/max_iter

# Part 3 - Making the predictions and evaluating the model

# Predicting the Test set results
# y_pred = classifier.predict(test_x)
y_pred = (test_preds > 0.5)

# Making the Confusion Matrix
from sklearn.metrics import confusion_matrix
cm = confusion_matrix(test_y.astype(int), y_pred)

# Calculate the Accuracy
from sklearn.metrics import accuracy_score
score=accuracy_score(y_pred,test_y.astype(int))

print("Test_Report:\n",classification_report(test_y.astype(int),y_pred))
cm

